{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to ../3. clean data/dataset_1.csv\n"
     ]
    }
   ],
   "source": [
    "#============================ Loading required libraries ============================#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "#============================ Step 1 Data Cleaning ============================#\n",
    "# Load the data\n",
    "file_path_framingham = '../2. datasets/Framingham_preprocessed_data.csv'\n",
    "file_path_stroke = '../2. datasets/StrokeHealthcareData.csv'\n",
    "file_path_clinical = '../2. datasets/HeartFailureClinicalRecords_data519.csv'\n",
    "file_path_statlog = '../2. datasets/StatlogHeart_data145.csv'\n",
    "file_path_cleveland = '../2. datasets/ClevelandData.csv'\n",
    "file_path_hungary = '../2. datasets/HungaryData.csv'\n",
    "file_path_swizerland = '../2. datasets/SwitzerlandData.csv'\n",
    "file_path_valongbeach = '../2. datasets/VaLongBeachData.csv'\n",
    "\n",
    "cleveland = pd.read_csv(file_path_cleveland)\n",
    "hungary = pd.read_csv(file_path_hungary)\n",
    "switzerland = pd.read_csv(file_path_swizerland)\n",
    "valongbeach = pd.read_csv(file_path_valongbeach)\n",
    "statlog = pd.read_csv(file_path_statlog)\n",
    "clinical = pd.read_csv(file_path_clinical)\n",
    "stroke = pd.read_csv(file_path_stroke)\n",
    "framingham = pd.read_csv(file_path_framingham)\n",
    "\n",
    "# Combine the datasets\n",
    "datasets_1 = [cleveland, hungary, switzerland, valongbeach]\n",
    "# datasets_2 = [statlog, clinical, stroke, framingham]\n",
    "datasets= [\n",
    "    df.drop(columns=['id', 'dataset'], errors='ignore') for df in datasets_1\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to ../3. clean data/dataset_2.csv\n",
      "Column: age\n",
      "Number of Unique Values: 50\n",
      "Unique Values: [63. 67. 37. 41. 56. 62. 57. 53. 44. 52. 48. 54. 49. 64. 58. 60. 50. 66.\n",
      " 43. 40. 69. 59. 42. 55. 61. 65. 71. 51. 46. 45. 39. 68. 47. 34. 35. 29.\n",
      " 70. 77. 38. 74. 76. 28. 30. 31. 32. 33. 36. 72. 73. 75.]\n",
      "\n",
      "Column: sex\n",
      "Number of Unique Values: 4\n",
      "Unique Values: ['Male' 'Female' 1.0 0.0]\n",
      "\n",
      "Column: chest-pain\n",
      "Number of Unique Values: 8\n",
      "Unique Values: ['typical angina' 'asymptomatic' 'non-anginal' 'atypical angina' 4.0 3.0\n",
      " 2.0 1.0]\n",
      "\n",
      "Column: rest-bps\n",
      "Number of Unique Values: 61\n",
      "Unique Values: [145. 160. 120. 130. 140. 172. 150. 110. 132. 117. 135. 112. 105. 124.\n",
      " 125. 142. 128. 170. 155. 104. 180. 138. 108. 134. 122. 115. 118. 100.\n",
      " 200.  94. 165. 102. 152. 101. 126. 174. 148. 178. 158. 192. 129. 144.\n",
      " 123. 136. 146. 106. 156. 154. 114. 164.  98. 190.  nan 113.  92.  95.\n",
      "  80. 185. 116.   0.  96. 127.]\n",
      "\n",
      "Column: serum-chol\n",
      "Number of Unique Values: 217\n",
      "Unique Values: [233. 286. 229. 250. 204. 236. 268. 354. 254. 203. 192. 294. 256. 263.\n",
      " 199. 168. 239. 275. 266. 211. 283. 284. 224. 206. 219. 340. 226. 247.\n",
      " 167. 230. 335. 234. 177. 276. 353. 243. 225. 302. 212. 330. 175. 417.\n",
      " 197. 198. 290. 253. 172. 273. 213. 305. 216. 304. 188. 282. 185. 232.\n",
      " 326. 231. 269. 267. 248. 360. 258. 308. 245. 270. 208. 264. 321. 274.\n",
      " 325. 235. 257. 164. 141. 252. 255. 201. 222. 260. 182. 303. 265. 309.\n",
      " 307. 249. 186. 341. 183. 407. 217. 288. 220. 209. 227. 261. 174. 281.\n",
      " 221. 205. 240. 289. 318. 298. 564. 246. 322. 299. 300. 293. 277. 214.\n",
      " 207. 223. 160. 394. 184. 315. 409. 244. 195. 196. 126. 313. 259. 200.\n",
      " 262. 215. 228. 193. 271. 210. 327. 149. 295. 306. 178. 237. 218. 242.\n",
      " 319. 166. 180. 311. 278. 342. 169. 187. 157. 176. 241. 131. 132.  nan\n",
      " 161. 173. 194. 297. 292. 339. 147. 291. 358. 412. 238. 163. 280. 202.\n",
      " 328. 129. 190. 179. 272. 100. 468. 320. 312. 171. 365. 344.  85. 347.\n",
      " 251. 287. 156. 117. 466. 338. 529. 392. 329. 355. 603. 404. 518. 285.\n",
      " 279. 388. 336. 491. 331. 393.   0. 153. 316. 458. 384. 349. 142. 181.\n",
      " 310. 170. 369. 165. 337. 333. 139. 385.]\n",
      "\n",
      "Column: fasting-blood-sugar\n",
      "Number of Unique Values: 2\n",
      "Unique Values: [True False nan]\n",
      "\n",
      "Column: restecg\n",
      "Number of Unique Values: 6\n",
      "Unique Values: ['lv hypertrophy' 'normal' 'st-t abnormality' nan 2.0 0.0 1.0]\n",
      "\n",
      "Column: max-heart-rate\n",
      "Number of Unique Values: 119\n",
      "Unique Values: [150. 108. 129. 187. 172. 178. 160. 163. 147. 155. 148. 153. 142. 173.\n",
      " 162. 174. 168. 139. 171. 144. 132. 158. 114. 151. 161. 179. 120. 112.\n",
      " 137. 157. 169. 165. 123. 128. 152. 140. 188. 109. 125. 131. 170. 113.\n",
      "  99. 177. 141. 180. 111. 143. 182. 156. 115. 149. 145. 146. 175. 186.\n",
      " 185. 159. 130. 190. 136.  97. 127. 154. 133. 126. 202. 103. 166. 164.\n",
      " 184. 124. 122.  96. 138.  88. 105. 194. 195. 106. 167.  95. 192. 117.\n",
      " 121. 116.  71. 118. 181. 134.  90.  98. 176. 135. 110.  nan 100.  87.\n",
      " 102.  92.  91.  82. 119.  94. 104.  60.  83.  63.  70.  77.  72.  78.\n",
      "  86.  93.  67.  84.  80. 107.  69.  73.]\n",
      "\n",
      "Column: exercise-angina\n",
      "Number of Unique Values: 2\n",
      "Unique Values: [False True nan]\n",
      "\n",
      "Column: st-depression\n",
      "Number of Unique Values: 53\n",
      "Unique Values: [ 2.3  1.5  2.6  3.5  1.4  0.8  3.6  0.6  3.1  0.4  1.3  0.   0.5  1.6\n",
      "  1.   1.2  0.2  1.8  3.2  2.4  2.   2.5  2.2  2.8  3.   3.4  6.2  4.\n",
      "  5.6  2.9  0.1  2.1  1.9  4.2  0.9  1.1  3.8  0.7  0.3  4.4  5.   nan\n",
      " -1.1 -1.5 -0.1 -2.6 -0.7 -2.  -1.   1.7 -0.8 -0.5 -0.9  3.7]\n",
      "\n",
      "Column: slope\n",
      "Number of Unique Values: 6\n",
      "Unique Values: ['downsloping' 'flat' 'upsloping' nan 2.0 1.0 3.0]\n",
      "\n",
      "Column: major-vessels\n",
      "Number of Unique Values: 4\n",
      "Unique Values: [ 0.  3.  2.  1. nan]\n",
      "\n",
      "Column: thal\n",
      "Number of Unique Values: 6\n",
      "Unique Values: ['fixed defect' 'normal' 'reversable defect' nan 3.0 7.0 6.0]\n",
      "\n",
      "Column: heart-disease\n",
      "Number of Unique Values: 5\n",
      "Unique Values: [0 2 1 3 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Replace 'cp' with 'chest pain' in the 'Symptom' column\n",
    "datasets_1 = datasets_1.rename(columns={'cp': 'chest-pain'})\n",
    "datasets_1 = datasets_1.rename(columns={'fbs': 'fasting-blood-sugar'})\n",
    "datasets_1 = datasets_1.rename(columns={'trestbps': 'rest-bps'})\n",
    "datasets_1 = datasets_1.rename(columns={'chol': 'serum-chol'})\n",
    "datasets_1 = datasets_1.rename(columns={'thalch': 'max-heart-rate'})\n",
    "datasets_1 = datasets_1.rename(columns={'ca': 'major-vessels'})\n",
    "datasets_1 = datasets_1.rename(columns={'resting-ecg': 'restecg'})\n",
    "datasets_1 = datasets_1.rename(columns={'exang': 'exercise-angina'})\n",
    "datasets_1 = datasets_1.rename(columns={'num': 'heart-disease'})\n",
    "datasets_1 = datasets_1.rename(columns={'oldpeak': 'st-depression'})\n",
    "\n",
    "\n",
    "\n",
    "datasets_2['sex'] = datasets_2['sex'].replace({'male': 1, 'female': 0})\n",
    "datasets_2['chest-pain'] = datasets_2['chest-pain'].replace({'typical angina': 0, 'atypical angina': 1, 'non-anginal pain': 2, 'atypical angina': 3})\n",
    "datasets_2['fasting-blood-sugar'] = datasets_2['fasting-blood-sugar'].replace({'TRUE': 1, 'FALSE': 0})\n",
    "datasets_2['restecg'] = datasets_2['restecg'].replace({'normal': 0, 'st-t abnormality': 1, 'lv hypertrophy': 2})\n",
    "datasets_2['exercise-angina'] = datasets_2['exercise-angina'].replace({'TRUE': 1, 'FALSE': 0})\n",
    "datasets_2['slope'] = datasets_2['slope'].replace({'male': 1, 'female': 0})\n",
    "datasets_2['thal'] = datasets_2['thal'].replace({'male': 1, 'female': 0})\n",
    "# Check the number of unique values and list the unique values for each column\n",
    "# for column in datasets_1.columns:\n",
    "#     unique_values = datasets_1[column].unique()  # Get unique values\n",
    "#     num_unique_values = datasets_1[column].nunique()  # Count unique values\n",
    "    \n",
    "#     print(f\"Column: {column}\")\n",
    "#     print(f\"Number of Unique Values: {num_unique_values}\")\n",
    "#     print(f\"Unique Values: {unique_values}\\n\")\n",
    "\n",
    "\n",
    "statlog = statlog.rename(columns={'rest-bp': 'rest-bps'})\n",
    "statlog = statlog.rename(columns={'electrocardiographic': 'restecg'})\n",
    "statlog = statlog.rename(columns={'angina': 'exercise-angina'})\n",
    "statlog = statlog.rename(columns={'oldpeak': 'st-depression'})\n",
    "\n",
    "datasets = [datasets_1, statlog]\n",
    "datasets_2 = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "output_path = \"../3. clean data/dataset_2.csv\"\n",
    "datasets_2.to_csv(output_path, index=False)\n",
    "print(f\"Combined dataset saved to {output_path}\")\n",
    "for column in datasets_2.columns:\n",
    "    unique_values = datasets_2[column].unique()  # Get unique values\n",
    "    num_unique_values = datasets_2[column].nunique()  # Count unique values\n",
    "    \n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Number of Unique Values: {num_unique_values}\")\n",
    "    print(f\"Unique Values: {unique_values}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical features\n",
    "# Handle missing values\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "#============================ Step 2 Generate a Common Feature Space ============================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: age\n",
      "Total Values 920\n",
      "Missing Values 0\n",
      "Percentage 0.0\n",
      "\n",
      "Column: sex\n",
      "Total Values 920\n",
      "Missing Values 0\n",
      "Percentage 0.0\n",
      "\n",
      "Column: chest-pain\n",
      "Total Values 920\n",
      "Missing Values 0\n",
      "Percentage 0.0\n",
      "\n",
      "Column: rest-bps\n",
      "Total Values 920\n",
      "Missing Values 59\n",
      "Percentage 6.41304347826087\n",
      "\n",
      "Column: serum-chol\n",
      "Total Values 920\n",
      "Missing Values 30\n",
      "Percentage 3.260869565217391\n",
      "\n",
      "Column: fasting-blood-sugar\n",
      "Total Values 920\n",
      "Missing Values 90\n",
      "Percentage 9.782608695652174\n",
      "\n",
      "Column: resting-ecg\n",
      "Total Values 920\n",
      "Missing Values 2\n",
      "Percentage 0.21739130434782608\n",
      "\n",
      "Column: max-heart-rate\n",
      "Total Values 920\n",
      "Missing Values 55\n",
      "Percentage 5.978260869565218\n",
      "\n",
      "Column: exercise-angina\n",
      "Total Values 920\n",
      "Missing Values 55\n",
      "Percentage 5.978260869565218\n",
      "\n",
      "Column: oldpeak\n",
      "Total Values 920\n",
      "Missing Values 62\n",
      "Percentage 6.739130434782608\n",
      "\n",
      "Column: slope\n",
      "Total Values 920\n",
      "Missing Values 309\n",
      "Percentage 33.58695652173913\n",
      "\n",
      "Column: major-vessels\n",
      "Total Values 920\n",
      "Missing Values 611\n",
      "Percentage 66.41304347826087\n",
      "\n",
      "Column: thal\n",
      "Total Values 920\n",
      "Missing Values 486\n",
      "Percentage 52.826086956521735\n",
      "\n",
      "Column: heart-disease\n",
      "Total Values 920\n",
      "Missing Values 0\n",
      "Percentage 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total missing values per column\n",
    "for column in datasets_1.columns:\n",
    "    missing_values = datasets_1[column].isnull().sum()\n",
    "    total_values = len(datasets_1[column])\n",
    "    missing_percentage = (datasets_1[column].isnull().sum() / len(datasets_1[column])) * 100\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Total Values {total_values}\")\n",
    "    print(f\"Missing Values {missing_values}\")\n",
    "    print(f\"Percentage {missing_percentage}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
